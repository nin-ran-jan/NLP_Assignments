{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Installing Sentence Transformer and other models/frameworks"
      ],
      "metadata": {
        "id": "PpcoqA-3l2g9"
      },
      "id": "PpcoqA-3l2g9"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "qsEYfs8JjZts",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7ed1cc0-2131-4fe2-d33a-6b2de1addacc"
      },
      "id": "qsEYfs8JjZts",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install sentence_transformers\n",
        "!pip install fasttext\n",
        "!pip install transformers \n",
        "# Kindly add all your installations and versions if any in this cell."
      ],
      "metadata": {
        "id": "0tw0xe31hT59",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3cff4e61-2d9c-4457-f3da-3ad3e7904a9a"
      },
      "id": "0tw0xe31hT59",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: fasttext in /usr/local/lib/python3.7/dist-packages (0.9.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fasttext) (1.21.6)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from fasttext) (57.4.0)\n",
            "Requirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.7/dist-packages (from fasttext) (2.10.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.24.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.11.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.13.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.10.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.9.24)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing necessary libraries. \n",
        "In the final version all imports should be stricly enlisted here."
      ],
      "metadata": {
        "id": "WWWTQtXDl-t8"
      },
      "id": "WWWTQtXDl-t8"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "e951f9be",
      "metadata": {
        "id": "e951f9be",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9600b53-ca12-4f6f-ee79-e20fa4e5fbc2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "###\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "# import spacy\n",
        "from scipy import stats\n",
        "from sklearn import linear_model\n",
        "\n",
        "# from sentence_transformers import SentenceTransformer, losses, models, util\n",
        "# from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
        "# from sentence_transformers.readers import InputExample\n",
        "\n",
        "import torch \n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "####\n",
        "\n",
        "import os\n",
        "import fasttext.util\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import csv\n",
        "\n",
        "from scipy.stats import spearmanr\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.preprocessing import StandardScaler  \n",
        "\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import BertModel\n",
        "from transformers import BertTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###\n",
        "os.chdir('/content/drive/MyDrive/')\n",
        "# os.mkdir('NLP_Ass3')\n",
        "os.chdir('NLP_Ass3')\n",
        "root = '/content/drive/MyDrive/NLP_Ass3'"
      ],
      "metadata": {
        "id": "VshdA7Qlj2_M"
      },
      "id": "VshdA7Qlj2_M",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load dataset: 7 marks\n",
        "1 Download and unzip the dataset from this link http://ixa2.si.ehu.es/stswiki/images/4/48/Stsbenchmark.tar.gz  **1 mark**\n",
        "\n",
        "2 Complete the code in `read_sts_csv()`. **4.5 marks**\n",
        "\n",
        "3 Create 3 dataframes one each for train, test and val and print their final shapes. **1.5 marks**"
      ],
      "metadata": {
        "id": "H5Ch9I58mMGe"
      },
      "id": "H5Ch9I58mMGe"
    },
    {
      "cell_type": "code",
      "source": [
        "# os.chdir(root)\n",
        "# !wget http://ixa2.si.ehu.es/stswiki/images/4/48/Stsbenchmark.tar.gz\n",
        "# !ls"
      ],
      "metadata": {
        "id": "H5gby7TnkX6v"
      },
      "id": "H5gby7TnkX6v",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(root)\n",
        "# !tar -xvf /content/drive/MyDrive/NLP_Ass3/Stsbenchmark.tar.gz\n",
        "#make a small edit in the .csv of 'sts-test.csv' - SOLVED\n",
        "#train.csv line number 2100 - SOLVED"
      ],
      "metadata": {
        "id": "gGXRxTHGk3Kp"
      },
      "id": "gGXRxTHGk3Kp",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###\n",
        "os.chdir(root)\n",
        "def read_sts_csv(dataset_type=\"train\", columns=['source', 'type', 'year', 'id', 'score', 'sent_a', 'sent_b']):\n",
        "    path = root+'/stsbenchmark/'+\"sts-\"+dataset_type+\".csv\"\n",
        "    return pd.read_csv(path, header=None, usecols=columns, names=columns, sep='\\t', quoting=csv.QUOTE_NONE)\n",
        "\n",
        "df_train = read_sts_csv() # create the train, dev and test dataframes\n",
        "df_dev = read_sts_csv('dev')\n",
        "df_test = read_sts_csv('test')\n",
        "\n",
        "print(\"Train data-frame shape\",df_train.to_numpy().shape)\n",
        "print(\"Dev data-frame shape\",df_dev.to_numpy().shape)\n",
        "print(\"Test data-frame shape\",df_test.to_numpy().shape)"
      ],
      "metadata": {
        "id": "2TMR0Z0DlfFf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "083c3de4-d654-4772-a7f4-15fef639320e"
      },
      "id": "2TMR0Z0DlfFf",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train data-frame shape (5749, 7)\n",
            "Dev data-frame shape (1500, 7)\n",
            "Test data-frame shape (1379, 7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperparameters: 5 Marks\n",
        "Update this cell with you choosen parameters except, NUM_EPOCHS"
      ],
      "metadata": {
        "id": "gO2ZkIwDmo4s"
      },
      "id": "gO2ZkIwDmo4s"
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(root)\n",
        "\n",
        "CONTEXTUAL_MODEL_TYPE = 'bert-base-uncased'\n",
        "# HUGGING_FACE_SENTENCE_TRANSFORMER_MODEL = <HUGGING_FACE_SENTENCE_TRANSFORMER_MODEL> # USE THE HUGGAING FACE VERSION OF SENTENCE_TRANSFORMER_TYPE\n",
        "# INPUT_PATH = <INPUT_FOLDER_PATH>\n",
        "BATCH_SIZE = 8\n",
        "# OUT_DIM_DENSE = <OUT_DIM_DENSE>\n",
        "NUM_EPOCHS = 2 ## THIS IS FIXED DO NOT CHANGE\n",
        "\n",
        "# You are free to add your own hyperparameters as well."
      ],
      "metadata": {
        "id": "4QurhOG7E0Z-"
      },
      "id": "4QurhOG7E0Z-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(root)\n",
        "\n",
        "# fasttext.util.download_model('en', if_exists='ignore')\n",
        "NON_CONTEXTUAL_MODEL_TYPE = fasttext.load_model('cc.en.300.bin') #fast-text as non-contextual model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "35Z9oVcfqraA",
        "outputId": "2d17341e-93f3-4a18-9822-31749dc397aa"
      },
      "id": "35Z9oVcfqraA",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CONFIGURATION 1: Non-contextual Embeddings + ML Regression: 8 marks\n",
        "1 Load the non-contextual embedding model in variable `non_cont_model1`. **1 marks**\n",
        "\n",
        "2 Get feature for the sentences using the LM model loaded before. Add the code in the `get_feature_model1()` **2 marks**\n",
        "\n",
        "2 Using features as X and score as Y, train a ML based regression model (`model1`). You are free to choose any sklearn based regression method, and its hyperparameters. **3.5 marks**\n",
        "\n",
        "3 Print the correlation scores on the dev and test set predictions using trained `model1`. **1.5 mark**\n",
        "\n"
      ],
      "metadata": {
        "id": "KgpbPlH9nXDy"
      },
      "id": "KgpbPlH9nXDy"
    },
    {
      "cell_type": "code",
      "source": [
        "def get_feature_model1(data_frame):\n",
        "    \"\"\"\n",
        "    Input a data frame and return the embedding vectors for the each sentence column using non_cont_model1,\n",
        "    Return 2 matrices each of shape (#_samples, #size_of_word_emb).\n",
        "    \"\"\"\n",
        "    dataset_np_array = data_frame.to_numpy()\n",
        "\n",
        "    embeddings_1 = []\n",
        "    for i in range(dataset_np_array.shape[0]):\n",
        "        word_tokens_1 = nltk.tokenize.word_tokenize(dataset_np_array[i][5])\n",
        "        mean_embedding_1 = np.zeros((300,)) #word embeddings in 300 dimensions\n",
        "        for token in word_tokens_1:\n",
        "            mean_embedding_1 += non_cont_model1.get_word_vector(token)\n",
        "        mean_embedding_1 /= len(word_tokens_1)\n",
        "        embeddings_1.append(mean_embedding_1)\n",
        "\n",
        "    embeddings_2 = []\n",
        "    for i in range(dataset_np_array.shape[0]):\n",
        "        word_tokens_2 = nltk.tokenize.word_tokenize(dataset_np_array[i][6])\n",
        "        mean_embedding_2 = np.zeros((300,)) #word embeddings in 300 dimensions\n",
        "        for token in word_tokens_2:\n",
        "            mean_embedding_2 += non_cont_model1.get_word_vector(token)\n",
        "        mean_embedding_2 /= len(word_tokens_2)\n",
        "        embeddings_2.append(mean_embedding_2)\n",
        "\n",
        "    return np.array(embeddings_1), np.array(embeddings_2)\n",
        "\n",
        "non_cont_model1 = NON_CONTEXTUAL_MODEL_TYPE\n",
        "\n",
        "feature_1_train, feature_2_train = get_feature_model1(df_train)\n",
        "print(\"Train shapes:\", feature_1_train.shape, feature_2_train.shape)\n",
        "feature_1_dev, feature_2_dev = get_feature_model1(df_dev)\n",
        "print(\"Dev shapes:\", feature_1_dev.shape, feature_2_dev.shape)\n",
        "feature_1_test, feature_2_test = get_feature_model1(df_test)\n",
        "print(\"Test shapes:\", feature_1_test.shape, feature_2_test.shape)"
      ],
      "metadata": {
        "id": "Hr7teQO9nfRR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "118c9d5d-5d0a-43e6-ef52-62eb03d691b9"
      },
      "id": "Hr7teQO9nfRR",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train shapes: (5749, 300) (5749, 300)\n",
            "Dev shapes: (1500, 300) (1500, 300)\n",
            "Test shapes: (1379, 300) (1379, 300)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# X_train, Y_train = np.concatenate((feature_1_train,feature_2_train), axis=1), df_train.to_numpy()[:,4]\n",
        "# X_dev, Y_dev = np.concatenate((feature_1_dev,feature_2_dev), axis=1), df_dev.to_numpy()[:,4]\n",
        "# X_test, Y_test = np.concatenate((feature_1_test,feature_2_test), axis=1), df_test.to_numpy()[:,4]\n",
        "\n",
        "# MAX_DEGREE = 1\n",
        "# NUM_COMPONENTS = 600\n",
        "\n",
        "# polyreg = make_pipeline(PolynomialFeatures(MAX_DEGREE, include_bias=True), LinearRegression())\n",
        "# pca = PCA(n_components=NUM_COMPONENTS)\n",
        "# X_train_pca = pca.fit_transform(X_train)\n",
        "# polyreg.fit(X_train_pca, Y_train)\n",
        "\n",
        "# X_dev_and_test_pca = pca.fit_transform(np.concatenate((X_dev, X_test)))\n",
        "# Y_predicted_dev_and_test = polyreg.predict(X_dev_and_test_pca)\n",
        "\n",
        "# print(\"Spearman correlation:\", stats.spearmanr(Y_predicted_dev_and_test, np.concatenate((Y_dev, Y_test))))\n",
        "# print(Y_predicted_dev_and_test.round(2))"
      ],
      "metadata": {
        "id": "q24aY3kIsBK7"
      },
      "id": "q24aY3kIsBK7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, Y_train = np.concatenate((feature_1_train,feature_2_train), axis=1), df_train.to_numpy()[:,4]\n",
        "X_dev, Y_dev = np.concatenate((feature_1_dev,feature_2_dev), axis=1), df_dev.to_numpy()[:,4]\n",
        "X_test, Y_test = np.concatenate((feature_1_test,feature_2_test), axis=1), df_test.to_numpy()[:,4]\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train)\n",
        "X_train = scaler.transform(X_train)\n",
        "X_dev = scaler.transform(X_dev)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "regr = MLPRegressor(random_state=42, batch_size=BATCH_SIZE, solver='adam', max_iter=NUM_EPOCHS).fit(X_train, Y_train)\n",
        "\n",
        "Y_predicted_dev = regr.predict(X_dev)\n",
        "Y_predicted_test = regr.predict(X_test)\n",
        "\n",
        "\n",
        "print(\"Spearman correlation dev:\", stats.spearmanr(Y_predicted_dev, Y_dev))\n",
        "print(\"Spearman correlation test:\", stats.spearmanr(Y_predicted_test, Y_test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RpCzEuRD0KwA",
        "outputId": "3399c4cb-66ef-488c-be87-b4498ddb569b"
      },
      "id": "RpCzEuRD0KwA",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spearman correlation dev: SpearmanrResult(correlation=0.25068476886281293, pvalue=6.284517507581251e-23)\n",
            "Spearman correlation test: SpearmanrResult(correlation=0.25313640089143147, pvalue=1.3275369550669051e-21)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CONFIGURATION 2: Contextual Embeddings + ML Regression: 7 marks\n",
        "1 Load the contextual embedding model in variable `non_cont_model2`. **1 marks**\n",
        "\n",
        "2 Get feature for the sentences using the LM model loaded before. Add the code in the `get_feature_model2()` **2 marks**\n",
        "\n",
        "2 Using features as X and score as Y, train a ML based regression model (`model2`). You are free to choose any sklearn based regression method, and its hyperparameters. **3.5 marks**\n",
        "\n",
        "3 Print the correlation scores on the dev and test set predictions using trained `model2`. **1.5 mark**\n",
        "\n",
        "Useful references: https://www.sbert.net/docs/usage/semantic_textual_similarity.html"
      ],
      "metadata": {
        "id": "DBzjbQ-grL8H"
      },
      "id": "DBzjbQ-grL8H"
    },
    {
      "cell_type": "code",
      "source": [
        "###\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, dataset, tokenizer):\n",
        "        self.scores = dataset[:,4]\n",
        "        self.sent_1 = []\n",
        "        self.sent_2 = []\n",
        "        for i in range(dataset.shape[0]):\n",
        "            self.sent_1.append(tokenizer(dataset[i,5], max_length=512, padding='max_length', truncation=True, return_tensors=\"pt\"))\n",
        "            self.sent_2.append(tokenizer(dataset[i,6], max_length=512, padding='max_length', truncation=True, return_tensors=\"pt\"))\n",
        "        # self.sent_1 = np.array(self.sent_1)\n",
        "        # self.sent_2 = np.array(self.sent_2)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.scores.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.scores[idx], self.sent_1[idx], self.sent_2[idx]\n",
        "\n",
        "    def getScores(self):\n",
        "        return self.scores"
      ],
      "metadata": {
        "id": "ze1LCeC7YlTe"
      },
      "id": "ze1LCeC7YlTe",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BertEmbeddingGenerator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BertEmbeddingGenerator, self).__init__()\n",
        "        self.bert = BertModel.from_pretrained(CONTEXTUAL_MODEL_TYPE)\n",
        "\n",
        "    def forward(self, input_ids, bert_mask):\n",
        "        return self.bert(input_ids=input_ids, attention_mask=bert_mask, return_dict=False)[1]"
      ],
      "metadata": {
        "id": "08alWqnEdhvb"
      },
      "id": "08alWqnEdhvb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def getEmbeddings(model, data, device):\n",
        "    model.eval()\n",
        "    if device=='cuda' and torch.cuda.is_available():\n",
        "        model.cuda()\n",
        "\n",
        "    dataloader = DataLoader(data, batch_size=BATCH_SIZE)\n",
        "\n",
        "    sents_1 = []\n",
        "    sents_2 = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch, (_, sent_1, sent_2) in enumerate(dataloader):\n",
        "            output_1 = model(sent_1[\"input_ids\"].squeeze(1).to(device), sent_1[\"attention_mask\"].to(device))\n",
        "            output_2 = model(sent_2[\"input_ids\"].squeeze(1).to(device), sent_2[\"attention_mask\"].to(device))\n",
        "            # print(output_1.shape)\n",
        "            for i in range(len(output_1)):\n",
        "                sents_1.append(output_1[i].cpu())\n",
        "                sents_2.append(output_2[i].cpu())\n",
        "    \n",
        "    sents_1 = np.array(sents_1)\n",
        "    sents_2 = np.array(sents_2)\n",
        "    print(sents_1.shape, sents_2.shape)\n",
        "\n",
        "    return sents_1, sents_2"
      ],
      "metadata": {
        "id": "5hAoF8y0fCZd"
      },
      "id": "5hAoF8y0fCZd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_feature_model2(data_frame):\n",
        "    \"\"\"\n",
        "    Input a data frame and return the embedding vectors for the each sentence column using model2,\n",
        "    Return 2 matrices each of shape (#_samples, #size_of_word_emb).\n",
        "    \"\"\"\n",
        "    BATCH_SIZE = 8\n",
        "    device = 'cuda'\n",
        "    model = BertEmbeddingGenerator()\n",
        "    # print(non_cont_model2)\n",
        "    dataset = CustomDataset(data_frame.to_numpy(), BertTokenizer.from_pretrained(non_cont_model2))\n",
        "    return getEmbeddings(model, dataset, device)\n",
        "\n",
        "\n",
        "\n",
        "non_cont_model2 = CONTEXTUAL_MODEL_TYPE\n",
        "\n",
        "feature_1_train, feature_2_train = get_feature_model2(df_train)\n",
        "print(\"Train shapes:\", feature_1_train.shape, feature_2_train.shape)\n",
        "feature_1_dev, feature_2_dev = get_feature_model2(df_dev)\n",
        "print(\"Dev shapes:\", feature_1_dev.shape, feature_2_dev.shape)\n",
        "feature_1_test, feature_2_test = get_feature_model2(df_test)\n",
        "print(\"Test shapes:\", feature_1_test.shape, feature_2_test.shape)"
      ],
      "metadata": {
        "id": "GlTVNjv0sNP0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "818411ac-c1f7-48ff-ff4e-08965153384a"
      },
      "id": "GlTVNjv0sNP0",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:20: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:20: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:21: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:21: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5749,) (5749,)\n",
            "Train shapes: (5749,) (5749,)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1500,) (1500,)\n",
            "Dev shapes: (1500,) (1500,)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1379,) (1379,)\n",
            "Test shapes: (1379,) (1379,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Y_train = df_train.to_numpy()[:,4]\n",
        "X_train = [np.concatenate((feature_1_train[i], feature_2_train[i]), axis=0) for i in range(feature_1_train.shape[0])]\n",
        "X_train = np.array(X_train)\n",
        "print(X_train.shape, Y_train.shape)\n",
        "Y_dev =  df_dev.to_numpy()[:,4]\n",
        "X_dev = [np.concatenate((feature_1_dev[i], feature_2_dev[i]), axis=0) for i in range(feature_1_dev.shape[0])]\n",
        "X_dev = np.array(X_dev)\n",
        "print(X_dev.shape, Y_dev.shape)\n",
        "Y_test = df_test.to_numpy()[:,4]\n",
        "X_test = [np.concatenate((feature_1_test[i], feature_2_test[i]), axis=0) for i in range(feature_1_test.shape[0])]\n",
        "X_test = np.array(X_test)\n",
        "print(X_test.shape, Y_test.shape)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train)\n",
        "X_train = scaler.transform(X_train)\n",
        "X_dev = scaler.transform(X_dev)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "BATCH_SIZE = 24\n",
        "\n",
        "regr = MLPRegressor(random_state=42, batch_size=BATCH_SIZE, solver='adam', max_iter=NUM_EPOCHS).fit(X_train, Y_train)\n",
        "Y_predicted_dev = regr.predict(X_dev)\n",
        "Y_predicted_test = regr.predict(X_test)\n",
        "\n",
        "\n",
        "print(\"Spearman correlation dev:\", stats.spearmanr(Y_predicted_dev, Y_dev))\n",
        "print(\"Spearman correlation test:\", stats.spearmanr(Y_predicted_test, Y_test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wN_enCSQ1zfG",
        "outputId": "e1b974ca-5693-46b5-fc64-bbf979b5e695"
      },
      "id": "wN_enCSQ1zfG",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5749, 1536) (5749,)\n",
            "(1500, 1536) (1500,)\n",
            "(1379, 1536) (1379,)\n",
            "Spearman correlation dev: SpearmanrResult(correlation=0.21481719768780774, pvalue=4.055571443668732e-17)\n",
            "Spearman correlation test: SpearmanrResult(correlation=0.2168585462890702, pvalue=3.8623145265734496e-16)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CONFIGURATION 3: Fine-Tune a Contextual Embeddings Model: 18 marks\n",
        "1 Prepare data samples to be for the DL model to consume. Add the code in the `form_data()`. **4 marks**\n",
        "\n",
        "3 Create the data loader, one each for train/dev/test data_input sample set obtained from `form_input_example()`. **1.5 marks**\n",
        "\n",
        "4 Initiate `model3` consisting of **atleast** the following 3 components - `base_LM`, a `pooling_layer` and a `dense_layer`. Use appropriate activation function in dense. **Atleast** one layer of `base_LM` should be set to trainable. **5 marks**\n",
        "\n",
        "6 Initiate the `loss`. **0.5 marks**\n",
        "\n",
        "7 Fit the `model3`. Use `NUM_EPOCHS = 2`. **MAX_NUM_EPOCHS allowed will be 3**. **2 marks** \n",
        "\n",
        "8 Complete the `get_model_predicts()` to obtain predicted scores for input sentence pairs. **3.5 marks** \n",
        "\n",
        "9 Print the correlation scores on the dev and test set predictions. **1.5 mark**\n",
        "\n",
        "Useful References: https://huggingface.co/blog/how-to-train-sentence-transformers "
      ],
      "metadata": {
        "id": "VImljTWps_GR"
      },
      "id": "VImljTWps_GR"
    },
    {
      "cell_type": "code",
      "source": [
        "###\n",
        "class BertSemantic(nn.Module):\n",
        "    def __init__(self): ## BERT POOL DENSE\n",
        "        super(BertSemantic, self).__init__()\n",
        "        self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
        "        self.avg_pool = nn.AvgPool1d(10, stride=2)\n",
        "        # self.linear_layer_1 = nn.LazyLinear(256)\n",
        "        self.linear_layer = nn.LazyLinear(256)\n",
        "        # self.linear_layer_2 = nn.Linear(32, 10)\n",
        "        # self.linear_layer_3 = nn.Linear(10, 1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.tan_h = nn.Tanh()\n",
        "        self.cos_sim = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
        "\n",
        "    def forward(self, input_ids_1, bert_mask_1, input_ids_2, bert_mask_2):\n",
        "        _, pooled_output_1 = self.bert(input_ids=input_ids_1, attention_mask=bert_mask_1, return_dict=False)\n",
        "        # print(pooled_output_1.shape)\n",
        "        pooled_cls_1 = self.avg_pool(pooled_output_1.reshape(1, pooled_output_1.shape[0], -1))\n",
        "        _, pooled_output_2 = self.bert(input_ids=input_ids_2, attention_mask=bert_mask_2, return_dict=False)\n",
        "        # print(pooled_output_1.shape)\n",
        "        pooled_cls_2 = self.avg_pool(pooled_output_2.reshape(1, pooled_output_2.shape[0], -1))\n",
        "\n",
        "        pooled_cls_1 = pooled_cls_1.squeeze(dim=0)\n",
        "        lin_cls_1 = self.tan_h(self.linear_layer(pooled_cls_1))\n",
        "\n",
        "        pooled_cls_2 = pooled_cls_2.squeeze(dim=0)\n",
        "        lin_cls_2 = self.tan_h(self.linear_layer(pooled_cls_2))\n",
        "\n",
        "        return self.cos_sim(lin_cls_1, lin_cls_2)\n",
        "        # return self.relu(self.linear_layer(pooled_cls))\n"
      ],
      "metadata": {
        "id": "w9REXtPMZY3K"
      },
      "id": "w9REXtPMZY3K",
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###\n",
        "\n",
        "def form_data(data_frame, data_type):\n",
        "    \"\"\"\n",
        "    Input a data frame and return the dataloder.\n",
        "    \"\"\"\n",
        "    # print(non_cont_model2)\n",
        "    dataset = CustomDataset(data_frame.to_numpy(), BertTokenizer.from_pretrained(\"bert-base-uncased\"))\n",
        "    if data_type == \"train\":\n",
        "        return DataLoader(dataset, batch_size=8, shuffle=True)\n",
        "    return DataLoader(dataset, batch_size=8)\n",
        "\n",
        "\n",
        "def get_model_predicts(data_type, trained_model):\n",
        "    \"\"\"\n",
        "    Input the dataset list and return a list of cosine similarity scores. Use the fitted final_trainable_model for obtaining encodings.\n",
        "    \"\"\"\n",
        "    BATCH_SIZE = 8\n",
        "    device = 'cuda'\n",
        "    if(data_type == \"dev\"):\n",
        "        return evalModel(trained_model, form_data(df_dev, \"dev\"), device)\n",
        "    elif(data_type == \"test\"):\n",
        "        return evalModel(trained_model, form_data(df_test, \"test\"), device)\n",
        "    elif(data_type == \"train\"):\n",
        "        return evalModel(trained_model, form_data(df_train, \"train\"), device)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def evalModel(model, dataloader, device):\n",
        "    model.eval()\n",
        "    if(device == \"cuda\"):\n",
        "        model.cuda()\n",
        "    \n",
        "    pred_scores = []\n",
        "    with torch.no_grad():\n",
        "        for batch, (scores, sent_1, sent_2) in enumerate(dataloader):\n",
        "            output = model(sent_1[\"input_ids\"].squeeze(1).to(device), sent_1[\"attention_mask\"].to(device), sent_2[\"input_ids\"].squeeze(1).to(device), sent_2[\"attention_mask\"].to(device))\n",
        "\n",
        "            for i in range(len(output)):\n",
        "                pred_scores.append(output[i].cpu())\n",
        "    model.train()\n",
        "    return  np.array(pred_scores)\n",
        "\n",
        "def trainModel(model, train_dataloader, learning_rate_bert, learning_rate, epochs, device):\n",
        "    model.train()\n",
        "    loss_function = nn.MSELoss()\n",
        "    optimizer_bert = torch.optim.Adam(model.parameters(), lr=learning_rate_bert)\n",
        "    optimizer_lin_1 = torch.optim.Adam(model3.linear_layer.parameters(), lr=learning_rate)\n",
        "    # optimizer_lin_2 = torch.optim.Adam(model3.linear_layer_2.parameters(), lr=learning_rate)\n",
        "    # optimizer_lin_3 = torch.optim.Adam(model3.linear_layer_3.parameters(), lr=learning_rate)\n",
        "\n",
        "    if(device == \"cuda\"):\n",
        "        model.cuda()\n",
        "        loss_function.cuda()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        for batch, (scores, sent_1, sent_2) in enumerate(train_dataloader):\n",
        "            output = model(sent_1[\"input_ids\"].squeeze(1).to(device), sent_1[\"attention_mask\"].to(device), sent_2[\"input_ids\"].squeeze(1).to(device), sent_2[\"attention_mask\"].to(device))\n",
        "            # print(\"output shape: \", output.shape)\n",
        "            # print(output.dtype)\n",
        "            loss = loss_function(output.squeeze(), scores.to(device).to(output.dtype).squeeze()/5)\n",
        "            # print(scores.to(device).dtype)\n",
        "            if batch%50 == 0:\n",
        "                print(f\"epoch: {epoch+1}, loss of current batch: {loss}\")\n",
        "            # update \n",
        "            optimizer_bert.zero_grad()\n",
        "            optimizer_lin_1.zero_grad()\n",
        "            # optimizer_lin_1.zero_grad()\n",
        "            # optimizer_lin_2.zero_grad()\n",
        "            # optimizer_lin_3.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer_bert.step()\n",
        "            optimizer_lin_1.step()\n",
        "            # optimizer_lin_2.step()\n",
        "            # optimizer_lin_3.step()\n",
        "\n",
        "        print(f\"EPOCH {epoch + 1} done!\")\n",
        "\n",
        "\n",
        "        pred_dev = get_model_predicts(\"dev\", model3)\n",
        "        pred_dev = [pred for pred in pred_dev]\n",
        "        Y_dev =  df_dev.to_numpy()[:,4]\n",
        "        print(\"Spearman correlation dev:\", stats.spearmanr(pred_dev, Y_dev))\n",
        "\n",
        "\n",
        "\n",
        "# # dataloader_<dataset_type> = form_data(data_frame)\n",
        "# dataloader_train = form_data(df_train, \"train\")\n",
        "# # base_model = \n",
        "# model3 = BertSemantic()\n",
        "# # layer_ppoling = \n",
        "# layer_dense = \n",
        "# model3 = \n",
        "# loss =\n",
        "\n"
      ],
      "metadata": {
        "id": "0kb0xJZmZGIR"
      },
      "id": "0kb0xJZmZGIR",
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader_train = form_data(df_train, \"train\")\n",
        "# base_model = \n",
        "model3 = BertSemantic()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "__M4Gm9mMAnQ",
        "outputId": "4c488c68-a38d-425d-c510-8c0322127f8c"
      },
      "id": "__M4Gm9mMAnQ",
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/lazy.py:178: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
            "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 205\n",
        "print(len([ para  for para in model3.parameters()]))\n",
        "print(len([ para  for para in model3.linear_layer.parameters()]))\n",
        "# print(len([ para  for para in model3.linear_layer_1.parameters()]))\n",
        "# print(len([ para  for para in model3.linear_layer_2.parameters()]))\n",
        "# print(len([ para  for para in model3.linear_layer_3.parameters()]))\n",
        "print(len([ para  for para in model3.bert.parameters()]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KTcC1_NzPIJf",
        "outputId": "5ffa59ec-ebf6-4c4d-834e-e1345e82eaa3"
      },
      "id": "KTcC1_NzPIJf",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "201\n",
            "2\n",
            "199\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the model3.\n",
        "trainModel(model3, dataloader_train, 0.00005, 0.001, 1, \"cuda\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 997
        },
        "id": "i6bJVz4po7iT",
        "outputId": "6b1d731e-0049-48b5-87a0-0ff1e24f34d4"
      },
      "id": "i6bJVz4po7iT",
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 1, loss of current batch: 0.08521401137113571\n",
            "epoch: 1, loss of current batch: 0.14950960874557495\n",
            "epoch: 1, loss of current batch: 0.33640211820602417\n",
            "epoch: 1, loss of current batch: 0.26294732093811035\n",
            "epoch: 1, loss of current batch: 0.09796900302171707\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Internal Python error in the inspect module.\n",
            "Below is the traceback from this internal error.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 3326, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-29-5ea2d2a3258a>\", line 2, in <module>\n",
            "    trainModel(model3, dataloader_train, 0.00005, 0.001, 1, \"cuda\")\n",
            "  File \"<ipython-input-27-60ac1b6463d6>\", line 72, in trainModel\n",
            "    loss.backward()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\", line 396, in backward\n",
            "    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\", line 175, in backward\n",
            "    allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 2040, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
            "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/ultratb.py\", line 319, in wrapped\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/ultratb.py\", line 353, in _fixed_getinnerframes\n",
            "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
            "  File \"/usr/lib/python3.7/inspect.py\", line 1502, in getinnerframes\n",
            "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
            "  File \"/usr/lib/python3.7/inspect.py\", line 1460, in getframeinfo\n",
            "    filename = getsourcefile(frame) or getfile(frame)\n",
            "  File \"/usr/lib/python3.7/inspect.py\", line 696, in getsourcefile\n",
            "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
            "  File \"/usr/lib/python3.7/inspect.py\", line 725, in getmodule\n",
            "    file = getabsfile(object, _filename)\n",
            "  File \"/usr/lib/python3.7/inspect.py\", line 709, in getabsfile\n",
            "    return os.path.normcase(os.path.abspath(_filename))\n",
            "  File \"/usr/lib/python3.7/posixpath.py\", line 383, in abspath\n",
            "    cwd = os.getcwd()\n",
            "FileNotFoundError: [Errno 2] No such file or directory\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred_dev = get_model_predicts(\"dev\", model3)\n",
        "pred_test = get_model_predicts(\"test\", model3)\n",
        "# pred_train = get_model_predicts(\"train\", model3)\n",
        "Y_dev =  df_dev.to_numpy()[:,4]\n",
        "# Y_train = df_train.to_numpy()[:,4]\n",
        "Y_test = df_test.to_numpy()[:,4]\n",
        "pred_dev = [pred for pred in pred_dev]\n",
        "pred_test = [pred for pred in pred_test]"
      ],
      "metadata": {
        "id": "BS1RKEgnpD-I"
      },
      "id": "BS1RKEgnpD-I",
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print spearman correlation on the predicted output of the dev and test sets.\n",
        "print(pred_dev[20:30])\n",
        "print(Y_dev.shape)\n",
        "print(\"Spearman correlation dev:\", stats.spearmanr(pred_dev, Y_dev, nan_policy='omit'))\n",
        "print(\"Spearman correlation test:\", stats.spearmanr(pred_test, Y_test, nan_policy='omit'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eJr4YXaMt3n0",
        "outputId": "4b70471b-c5c4-40e6-c0f2-cae2007b8f1f"
      },
      "id": "eJr4YXaMt3n0",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.943133, 0.7712677, 0.67079604, 0.504155, 0.30420354, 0.80869627, 0.8496884, 0.5772686, 0.36969975, 0.12360539]\n",
            "(1500,)\n",
            "Spearman correlation dev: SpearmanrResult(correlation=0.6816948352135648, pvalue=1.5658668824475524e-205)\n",
            "Spearman correlation test: SpearmanrResult(correlation=0.6267292332147769, pvalue=2.3115458342051015e-151)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(pred_dev)\n",
        "# print(pred_dev.shape)\n",
        "# print(pred_dev.numpy())\n",
        "# print(Y_dev.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h4UQ7oE6toJr",
        "outputId": "c8362915-9ea7-433c-ac65-c9a3eeaf5db7"
      },
      "id": "h4UQ7oE6toJr",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[array([0.], dtype=float32), array([0.], dtype=float32), array([0.], dtype=float32), array([0.], dtype=float32), array([0.], dtype=float32), array([0.], dtype=float32), array([0.], dtype=float32), array([0.], dtype=float32), array([0.], dtype=float32), array([0.], dtype=float32), array([0.], dtype=float32), array([0.], dtype=float32), array([0.], dtype=float32), array([0.], dtype=float32), array([0.], dtype=float32), array([0.], dtype=float32), array([0.], dtype=float32), array([0.], dtype=float32), array([0.], dtype=float32), array([0.], dtype=float32), array([0.], dtype=float32), array([0.], dtype=float32), array([0.], dtype=float32), array([0.], dtype=float32), array([0.], dtype=float32), array([0.], dtype=float32), array([0.], dtype=float32), array([0.], dtype=float32), array([0.], dtype=float32), array([0.], dtype=float32), array([0.], dtype=float32), array([0.], dtype=float32), array([0.], dtype=float32), array([0.], dtype=float32), array([0.], dtype=float32), array([0.], dtype=float32), array([0.], dtype=float32), array([0.], dtype=float32), array([0.], dtype=float32), array([0.], dtype=float32), array([0.], dtype=float32), array([0.], dtype=float32), array([0.], dtype=float32), array([0.], dtype=float32), array([0.], dtype=float32), array([0.], dtype=float32), array([0.], dtype=float32), array([0.], dtype=float32), array([0.], dtype=float32), array([0.], dtype=float32), array([0.], dtype=float32), array([0.], dtype=float32), array([0.], dtype=float32), array([0.], dtype=float32), array([0.], dtype=float32), array([0.], dtype=float32), array([0.], dtype=float32), array([0.], dtype=float32), array([0.], dtype=float32), array([0.], dtype=float32), array([0.], dtype=float32), array([0.], dtype=float32), array([0.], dtype=float32), array([0.], dtype=float32), array([0.], dtype=float32), array([0.], dtype=float32), array([0.], dtype=float32), array([0.], dtype=float32), array([0.], dtype=float32), array([0.], dtype=float32), array([0.], dtype=float32), array([0.], dtype=float32), array([0.], dtype=float32), array([0.], dtype=float32), array([0.], dtype=float32), array([0.], dtype=float32), array([0.], dtype=float32), array([0.], dtype=float32), array([0.], dtype=float32), array([0.], dtype=float32), array([0.], dtype=float32), array([0.], dtype=float32), array([0.], dtype=float32), array([0.], dtype=float32), array([0.], dtype=float32), array([0.], dtype=float32), array([0.], dtype=float32), array([0.], dtype=float32), array([0.], dtype=float32), array([0.], dtype=float32), array([0.], dtype=float32), array([0.], dtype=float32), array([0.], dtype=float32), array([0.], dtype=float32), array([0.], dtype=float32), array([0.], dtype=float32), array([0.], dtype=float32), array([0.], dtype=float32), array([0.], dtype=float32), array([0.], dtype=float32)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o1WVLn2mi6QG",
        "outputId": "fd4f93e4-8925-4621-f175-7d75837b9b56"
      },
      "id": "o1WVLn2mi6QG",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Nov 28 09:37:54 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   53C    P8    10W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}